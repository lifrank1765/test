{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090341bd-e4d0-4ccc-bc3d-bc16035e5594",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://timeseriesai.github.io/tsai/models.tst.html\n",
    "CNN and transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a61e6-81dd-4d61-83dd-63348640b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e2970-4252-485e-8f5e-7d703b73d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename, delimiter=\"\\t\")\n",
    "    y = data[:, 0]\n",
    "    x = data[:, 1:]\n",
    "    return x, y.astype(int)\n",
    "\n",
    "\n",
    "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
    "\n",
    "x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
    "x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "idx = np.random.permutation(len(x_train))\n",
    "x_train = x_train[idx]\n",
    "y_train = y_train[idx]\n",
    "\n",
    "y_train[y_train == -1] = 0\n",
    "y_test[y_test == -1] = 0\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "transformed_y_train = mlb.fit_transform(np.expand_dims(y_train, axis=1))\n",
    "\n",
    "transformed_y_test = mlb.fit_transform(np.expand_dims(y_train, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947817e9-aa74-46cd-8d0c-fe34f69a17f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c8d1fa-7f2c-4dc4-a406-f041d762ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim,\n",
    "                        dropout=0, attention_axes=None):\n",
    "  \"\"\"\n",
    "  Creates a single transformer block.\n",
    "  \"\"\"\n",
    "  x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "  x = layers.MultiHeadAttention(\n",
    "      key_dim=head_size, num_heads=num_heads, dropout=dropout,\n",
    "      attention_axes=attention_axes\n",
    "      )(x, x)\n",
    "  x = layers.Dropout(dropout)(x)\n",
    "  res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "  x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "  x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "  x = layers.Dropout(dropout)(x)\n",
    "  x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "  return x + res\n",
    "\n",
    "def build_transfromer(input_shape,n_outputs,\n",
    "                      head_size, \n",
    "                      num_heads,\n",
    "                      ff_dim,\n",
    "                      num_trans_blocks,\n",
    "                      mlp_units, dropout=0, mlp_dropout=0) -> tf.keras.Model:\n",
    "  \"\"\"\n",
    "  Creates final model by building many transformer blocks.\n",
    "  \"\"\"\n",
    "  inputs = tf.keras.Input(shape=input_shape)\n",
    "  x = inputs \n",
    "  for _ in range(num_trans_blocks):\n",
    "    x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "  \n",
    "  x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "  for dim in mlp_units:\n",
    "    x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(mlp_dropout)(x)\n",
    "\n",
    "  outputs = layers.Dense(n_outputs, activation='relu')(x)\n",
    "  return tf.keras.Model(inputs, outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ad312f-e0c0-4fe3-992e-fa3733682246",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "\n",
    "\n",
    "model = build_transfromer(input_shape,2,head_size=128, num_heads=2, ff_dim=128, \n",
    "                                num_trans_blocks=4, mlp_units=[256], \n",
    "                                mlp_dropout=0.10, dropout=0.10, )\n",
    "model.summary()\n",
    "'''\n",
    "model.compile(\n",
    "    loss=\"mse\",\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[\"mae\", 'mape'],\n",
    ")\n",
    "'''\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n",
    "                      metrics=['accuracy'])\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "#t_hist = model.fit(x_train, transformed_y_train, batch_size=32,\n",
    "#                         epochs=25, validation_data=(x_test, transformed_y_test),\n",
    "#                         verbose=1, callbacks=callbacks)\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    transformed_y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=150,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "'''\n",
    "\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=120,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.2,\n",
    "    dropout=0.25,\n",
    ")\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n",
    "                      metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    transformed_y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=150,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model.evaluate(x_test, transformed_y_test, verbose=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef89a50-f4e2-438e-a28f-9595fb794205",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c1ed65-2eea-4ea7-b231-e55ff2a973a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d37c14b-4cdf-4329-b314-ca05470698c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model11.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e51d43-51ef-4a7e-9507-f38bf657e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "class Classifier_INCEPTION:\n",
    "\n",
    "    def __init__(self, input_shape, nb_classes, verbose=True, build=True, batch_size=64,\n",
    "                 nb_filters=32, use_residual=True, use_bottleneck=True, depth=3, kernel_size=7, nb_epochs=150):\n",
    "\n",
    "        self.output_directory = \"\"\n",
    "\n",
    "        self.nb_filters = nb_filters\n",
    "        self.use_residual = use_residual\n",
    "        self.use_bottleneck = use_bottleneck\n",
    "        self.depth = depth\n",
    "        self.kernel_size = kernel_size - 1\n",
    "        self.callbacks = None\n",
    "        self.batch_size = batch_size\n",
    "        self.bottleneck_size = 32\n",
    "        self.nb_epochs = nb_epochs\n",
    "\n",
    "        if build == True:\n",
    "            self.model = self.build_model(input_shape, nb_classes)\n",
    "            if (verbose == True):\n",
    "                self.model.summary()\n",
    "            self.verbose = verbose\n",
    "            #self.model.save_weights(self.output_directory + 'model_init.hdf5')\n",
    "\n",
    "    def _inception_module(self, input_tensor, stride=1, activation='linear'):\n",
    "\n",
    "        if self.use_bottleneck and int(input_tensor.shape[-1]) > 1:\n",
    "            input_inception = keras.layers.Conv1D(filters=self.bottleneck_size, kernel_size=1,\n",
    "                                                  padding='same', activation=activation, use_bias=False)(input_tensor)\n",
    "        else:\n",
    "            input_inception = input_tensor\n",
    "\n",
    "        # kernel_size_s = [3, 5, 8, 11, 17]\n",
    "        kernel_size_s = [self.kernel_size // (2 ** i) for i in range(3)]\n",
    "\n",
    "        conv_list = []\n",
    "\n",
    "        for i in range(len(kernel_size_s)):\n",
    "            conv_list.append(keras.layers.Conv1D(filters=self.nb_filters, kernel_size=kernel_size_s[i],\n",
    "                                                 strides=stride, padding='same', activation=activation, use_bias=False)(\n",
    "                input_inception))\n",
    "\n",
    "        max_pool_1 = keras.layers.MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n",
    "\n",
    "        conv_6 = keras.layers.Conv1D(filters=self.nb_filters, kernel_size=1,\n",
    "                                     padding='same', activation=activation, use_bias=False)(max_pool_1)\n",
    "\n",
    "        conv_list.append(conv_6)\n",
    "\n",
    "        x = keras.layers.Concatenate(axis=2)(conv_list)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Activation(activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def _shortcut_layer(self, input_tensor, out_tensor):\n",
    "        shortcut_y = keras.layers.Conv1D(filters=int(out_tensor.shape[-1]), kernel_size=1,\n",
    "                                         padding='same', use_bias=False)(input_tensor)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "        x = keras.layers.Add()([shortcut_y, out_tensor])\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    def build_model(self, input_shape, nb_classes):\n",
    "        input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "        x = input_layer\n",
    "        input_res = input_layer\n",
    "\n",
    "        for d in range(self.depth):\n",
    "\n",
    "            x = self._inception_module(x)\n",
    "\n",
    "            if self.use_residual and d % 3 == 2:\n",
    "                x = self._shortcut_layer(input_res, x)\n",
    "                input_res = x\n",
    "\n",
    "        gap_layer = keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "        output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
    "\n",
    "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50,\n",
    "                                                      min_lr=0.0001)\n",
    "\n",
    "        file_path = self.output_directory + 'best_model.hdf5'\n",
    "\n",
    "        model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
    "                                                           save_best_only=True)\n",
    "\n",
    "        self.callbacks = [reduce_lr, model_checkpoint]\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, x_train, y_train, x_val, y_val, y_true, plot_test_acc=False):\n",
    "        if len(keras.backend.tensorflow_backend._get_available_gpus()) == 0:\n",
    "            print('error no gpu')\n",
    "            exit()\n",
    "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
    "\n",
    "        if self.batch_size is None:\n",
    "            mini_batch_size = int(min(x_train.shape[0] / 10, 16))\n",
    "        else:\n",
    "            mini_batch_size = self.batch_size\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        if plot_test_acc:\n",
    "\n",
    "            hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=self.nb_epochs,\n",
    "                                  verbose=self.verbose, validation_data=(x_val, y_val), callbacks=self.callbacks)\n",
    "        else:\n",
    "\n",
    "            hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=self.nb_epochs,\n",
    "                                  verbose=self.verbose, callbacks=self.callbacks)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        self.model.save(self.output_directory + 'last_model.hdf5')\n",
    "\n",
    "        y_pred = self.predict(x_val, y_true, x_train, y_train, y_val,\n",
    "                              return_df_metrics=False)\n",
    "\n",
    "        # save predictions\n",
    "        np.save(self.output_directory + 'y_pred.npy', y_pred)\n",
    "\n",
    "        # convert the predicted from binary to integer\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    def predict(self, x_test, y_true, x_train, y_train, y_test, return_df_metrics=True):\n",
    "        start_time = time.time()\n",
    "        model_path = self.output_directory + 'best_model.hdf5'\n",
    "        model = keras.models.load_model(model_path)\n",
    "        y_pred = model.predict(x_test, batch_size=self.batch_size)\n",
    "        return y_pred\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ca491-c5ba-475b-89d3-089254fcc8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "transformed_y = mlb.fit_transform(np.expand_dims(y_train, axis=1))\n",
    "\n",
    "print(transformed_y)\n",
    "\n",
    "a = Classifier_INCEPTION((500,1),2)\n",
    "\n",
    "a.model.fit(\n",
    "    x_train,\n",
    "    transformed_y,\n",
    "    validation_split=0.2,\n",
    "    epochs=150,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df36e3-6c8f-4a35-9171-0c6b8d428d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(a.model)\n",
    "tflite_model = converter.convert()\n",
    "with open(\"model2.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41eeb6-856d-4101-bb82-71b862411149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, AveragePooling1D, Concatenate, Dropout, BatchNormalization, Activation, GlobalAveragePooling1D, Dense\n",
    "\n",
    "def conv1d_bn(x, filters, kernel_size, padding='same', strides=1, activation='relu'):\n",
    "    x = Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def inception_module(x, filters):\n",
    "    conv1x1 = conv1d_bn(x, filters=filters[0], kernel_size=3)\n",
    "\n",
    "    conv3x3_reduce = conv1d_bn(x, filters=filters[1], kernel_size=1)\n",
    "    conv3x3 = conv1d_bn(conv3x3_reduce, filters=filters[2], kernel_size=5)\n",
    "\n",
    "    conv5x5_reduce = conv1d_bn(x, filters=filters[3], kernel_size=1)\n",
    "    conv5x5 = conv1d_bn(conv5x5_reduce, filters=filters[4], kernel_size=7)\n",
    "\n",
    "    maxpool = MaxPooling1D(pool_size=3, strides=1, padding='same')(x)\n",
    "    maxpool_conv = conv1d_bn(maxpool, filters=filters[5], kernel_size=1)\n",
    "\n",
    "    output = Concatenate(axis=-1)([conv1x1, conv3x3, conv5x5, maxpool_conv])\n",
    "    return output\n",
    "\n",
    "def InceptionTime(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = inception_module(inputs, filters=[32, 32, 32, 32, 32, 32])\n",
    "    x = inception_module(x, filters=[32, 32, 32, 32, 32, 32])\n",
    "    #x = inception_module(x, filters=[32, 32, 32, 32, 32, 32])\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape = (500, 1)  # Define your input shape\n",
    "num_classes = 2  # Define the number of classes for classification\n",
    "\n",
    "model = InceptionTime(input_shape, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d863a88-2f62-4d70-bf3c-706ce523cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=150,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca2f12-ccdf-4c09-b5f2-ed45159f3508",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with open(\"model3.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4748a1dd-3a49-4cfa-9bf7-e6b652394f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(object):\n",
    "    \"\"\" Building the Recurrent Neural Network for Multivariate time series forecasting\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialization of the object\n",
    "        \"\"\"\n",
    "        # Get model hyperparameters\n",
    "        self.look_back = 500\n",
    "        self.n_features = 1\n",
    "        self.horizon = 2\n",
    "\n",
    "        self.checkpoint_dir = \"ZZZ\"\n",
    "\n",
    "        self.head_size=128\n",
    "        self.num_heads=8\n",
    "        self.ff_dim=128\n",
    "        self.num_transformer_blocks=2\n",
    "        self.mlp_units=[64]\n",
    "        self.mlp_dropout=0.4\n",
    "        self.dropout=0.25\n",
    "\n",
    "\n",
    "    def transformer_encoder(self,\n",
    "        inputs):\n",
    "\n",
    "        # Normalization and Attention\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        x = layers.MultiHeadAttention(\n",
    "        key_dim=self.head_size, num_heads=self.num_heads, dropout=self.dropout)(x, x)\n",
    "        x = layers.Dropout(self.dropout)(x)\n",
    "\n",
    "        res = x + inputs\n",
    "\n",
    "        # Feed Forward Part\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "        x = layers.Conv1D(filters=self.ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(self.dropout)(x)\n",
    "        x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "        return x + res\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Build the model architecture\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = keras.Input(shape=(self.look_back, self.n_features))\n",
    "        x = inputs\n",
    "        for _ in range(self.num_transformer_blocks):\n",
    "            x = self.transformer_encoder(x)\n",
    "\n",
    "        x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "        for dim in self.mlp_units:\n",
    "            x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "            x = layers.Dropout(self.mlp_dropout)(x)\n",
    "\n",
    "        # output layer\n",
    "        outputs = layers.Dense(self.horizon,activation='softmax')(x)\n",
    "\n",
    "        return keras.Model(inputs, outputs)\n",
    "\n",
    "    def train(self,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=200,\n",
    "        batch_size=64):\n",
    "        \"\"\" Training the network\n",
    "        :param X_train: training feature vectors [#batch,#number_of_timesteps,#number_of_features]\n",
    "        :type 3-D Numpy array of float values\n",
    "        :param Y_train: training target vectors\n",
    "        :type 2-D Numpy array of float values\n",
    "        :param epochs: number of training epochs\n",
    "        :type int\n",
    "        :param batch_size: size of batches used at each forward/backward propagation\n",
    "        :type int\n",
    "        :return -\n",
    "        :raises: -\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = self.build()\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        print(self.model.summary())\n",
    "\n",
    "        # Stop training if error does not improve within 50 iterations\n",
    "        early_stopping_monitor = EarlyStopping(patience=50, restore_best_weights=True)\n",
    "\n",
    "        # Save the best model ... with minimal error\n",
    "        filepath = self.checkpoint_dir+\"/Transformer.best\"+datetime.now().strftime('%d%m%Y_%H:%M:%S')+\".hdf5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "        callback_history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                             validation_split=0.2,\n",
    "                             verbose=1,\n",
    "                             callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])\n",
    "                             #callbacks=[PlotLossesKeras(), early_stopping_monitor, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97731f-7b15-41f3-a87b-5e27bcfca96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Transformer()\n",
    "b.train(x_train,transformed_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa162d21-2a95-49ab-8b31-8515ac933301",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(b.model)\n",
    "tflite_model = converter.convert()\n",
    "with open(\"model3.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c849b-d9ee-4ba0-8ed1-88addf5ba4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
