{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Motion Sensor Data into Pandas DataFrame\n",
    "The following Python code imports necessary libraries and combines motion sensor data from multiple CSV files, storing it in a structured Pandas DataFrame. Subject-specific information, such as age, gender, height, and weight, is added to the dataset for comprehensive analysis.\n",
    "\n",
    "* Imports the necessary libraries, including 'os' for file operations, 'numpy' as 'np' for numerical operations, and 'pandas' as 'pd' for data handling.\n",
    "* Specifies the path to the subject data file and the directory containing motion sensor data.\n",
    "* Defines two functions:\n",
    "    'get_all_dataset_paths' that recursively walks through the specified directory and collects paths to all CSV files.\n",
    "    'load_whole_dataframe_from_paths' that reads and combines motion sensor data from these paths into a single Pandas DataFrame. It also enriches the data with subject information from the subject data file.\n",
    "* Loads the subject data from the CSV file 'data_subjects_info.csv' into a Pandas DataFrame.\n",
    "* Calls 'get_all_dataset_paths' to obtain a list of paths to all CSV files in the specified directory.\n",
    "* Calls 'load_whole_dataframe_from_paths' to create a comprehensive DataFrame containing motion sensor data, with additional subject information.\n",
    "\n",
    "This code is a critical step in preparing motion sensor data for analysis and is commonly used in data science and machine learning projects involving motion data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# change these following three lines only\n",
    "subject_data_file = 'data_subjects_info.csv'\n",
    "data_dir = 'E:/motion-sense-master/data/A_DeviceMotion_data'\n",
    "\n",
    "os.chdir(data_dir)\n",
    "os.chdir(os.pardir)\n",
    "\n",
    "def get_all_dataset_paths(input_dir) -> []:\n",
    "    input_files = []\n",
    "    for dirs, subdirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                input_files.append(os.path.join(dirs, file))\n",
    "    return input_files\n",
    "\n",
    "def load_whole_dataframe_from_paths(paths, meta) -> pd.DataFrame:\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for p in paths:\n",
    "        p = p.replace(\"\\\\\",'/')\n",
    "        c_dir, c_file = p.split('/')[-2], p.split('/')[-1]\n",
    "        \n",
    "        c_cat, c_ses = c_dir.split('_')[-2], c_dir.split('_')[-1]\n",
    "        c_sub = c_file.split('_')[-1].split('.')[-2]\n",
    "        \n",
    "        tdf = pd.read_csv(p, encoding = \"utf-8\")\n",
    "        tdf = tdf.assign(subject_id = int(c_sub))\n",
    "        tdf = tdf.assign(session_id = int(c_ses))\n",
    "        tdf = tdf.assign(category = str(c_cat))\n",
    "        tdf = tdf.assign(age = int(meta.age[int(c_sub) - 1]))\n",
    "        tdf = tdf.assign(gender = int(meta.gender[int(c_sub) - 1]))\n",
    "        tdf = tdf.assign(height = int(meta.height[int(c_sub) - 1]))\n",
    "        tdf = tdf.assign(weight = int(meta.weight[int(c_sub) - 1]))\n",
    "\n",
    "        df = pd.concat([df, tdf])\n",
    "        print(p,c_cat,c_sub)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "subject_data_frame = pd.DataFrame(pd.read_csv(subject_data_file, encoding = \"utf-8\"))\n",
    "\n",
    "\n",
    "all_dataset_paths = get_all_dataset_paths(data_dir)\n",
    "random.Random(4).shuffle(all_dataset_paths)\n",
    "\n",
    "data_frame = load_whole_dataframe_from_paths(all_dataset_paths, subject_data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full DataFrame at a glance\n",
    "The whole raw DataFrame looks like the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Removing Unnecessary Columns\n",
    "In this Python code, a copy of the original DataFrame 'data_frame' is created. Subsequently, several columns ('Unnamed: 0', 'subject_id', 'session_id', 'age', 'gender', 'height', and 'weight') are removed from the copied DataFrame 'df' to streamline the dataset for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_frame.copy() #making a copy of original dataframe\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.drop('subject_id', axis=1, inplace=True)\n",
    "df.drop('session_id', axis=1, inplace=True)\n",
    "df.drop('age', axis=1, inplace=True)\n",
    "df.drop('gender', axis=1, inplace=True)\n",
    "df.drop('height', axis=1, inplace=True)\n",
    "df.drop('weight', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Data for Machine Learning\n",
    "Following Python code snippet utilizes the 'LabelEncoder' from the scikit-learn library to transform the 'category' column in the DataFrame 'df' into numerical codes. These codes are stored in a new 'code' column, and the original 'category' column is subsequently removed from the DataFrame, preparing the data for machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lEncoder = LabelEncoder()\n",
    "labels = lEncoder.fit(df.category)\n",
    "df['code'] = lEncoder.transform(df.category)\n",
    "df.drop('category', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Categorical Data Distribution\n",
    "We use Seaborn and Matplotlib to create a countplot, visualizing the distribution of numerical codes in the 'code' column of the DataFrame 'df.' This plot provides insight into the frequency of different categories in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "sns.countplot(df, x='code')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data for Machine Learning\n",
    "The following code uses the 'train_test_split' function from scikit-learn to divide the dataset into training and testing sets. It separates the input features ('x_columns') and the target variable ('y_columns') with a 20% test set size, ensuring that the lengths of the training sets for both features and labels are the same, as asserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_columns = df.iloc[:, 0:12]\n",
    "y_columns = df.iloc[:, 12:13]\n",
    "\n",
    "trainx, testx, trainy, testy = train_test_split(x_columns, y_columns, test_size=0.2, shuffle=False)\n",
    "assert(len(trainx) == len(trainy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequencing Data for Temporal Analysis\n",
    "We define a sequence generator function that creates sequences of input features and corresponding target labels from the training and testing data. These sequences have a window length of 150 with a stride of 10. The mode of target labels within each sequence is calculated to represent the label for that sequence. This prepares the data for temporal analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "WINDOW_LENGTH = 150\n",
    "STRIDE_LENGTH = 10\n",
    "NUM_CLASSES = 6\n",
    "NUM_FEATURES = 12\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS_SIZE = 10\n",
    "\n",
    "def sequence_generator(x, y, length, stride):\n",
    "    seq_x = []\n",
    "    seq_y = []\n",
    "    data_length = len(x)\n",
    "\n",
    "    for i in range(0, data_length - length + 1, stride):\n",
    "        input_sequence = x.iloc[i : i + length]\n",
    "        target_sequence = y.iloc[i : i + length]\n",
    "        target_mode = mode(target_sequence.values)[0][0]\n",
    "        seq_x.append(input_sequence)\n",
    "        seq_y.append(target_mode)\n",
    "    return np.array(seq_x), np.array(seq_y)\n",
    "\n",
    "tx, ty = sequence_generator(trainx, trainy, WINDOW_LENGTH, STRIDE_LENGTH)\n",
    "vx, vy = sequence_generator(testx, testy, WINDOW_LENGTH, STRIDE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tsai.all import *\n",
    "X = np.concatenate([tx,vx])\n",
    "y = np.concatenate([ty,vy])\n",
    "splits = [[i for i in range(ty.shape[0])],[i for i in range(ty.shape[0],y.shape[0])] ]\n",
    "\n",
    "\n",
    "tfms  = [None, [Categorize()]]\n",
    "dsets = TSDatasets(X, y, tfms=tfms, splits=splits)\n",
    " \n",
    "bs = 256\n",
    "dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[bs, bs*2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "archs = [\n",
    "         (RNNPlus, {'n_layers':3, 'bidirectional': True} ),\n",
    "         (LSTMPlus,{'n_layers':3, 'bidirectional': True} ),\n",
    "         (GRUPlus, {'n_layers':3, 'bidirectional': True} ),   \n",
    "         (RNNPlus, {'n_layers':4, 'bidirectional': True} ),\n",
    "         (RNNPlus, {'n_layers':4, 'bidirectional': False}),  \n",
    "         (LSTM,    {'n_layers':3, 'bidirectional': False}), \n",
    "         (RNN,     {'n_layers':3, 'bidirectional': True} ), \n",
    "         (LSTM,    {'n_layers':3, 'bidirectional': True} ),\n",
    "         (GRU,     {'n_layers':3, 'bidirectional': True} ),   \n",
    "         (ResNet, {}), \n",
    "         (xresnet1d34, {}), \n",
    "         (xresnet1d50_deeper, {}), \n",
    "         (InceptionTime, {}), \n",
    "         (XceptionTime,  {}), \n",
    "         (TCN, {}),   \n",
    "         (LSTM_FCN, {}), \n",
    "         (TST, {}),\n",
    "         (FCN, {}),   \n",
    "        ]\n",
    "'''\n",
    "#         (FCN, {}), \n",
    "archs = [\n",
    "         (TST, {}),\n",
    "  \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "results = pd.DataFrame(columns=['arch', 'hyperparams', 'total params', 'train loss', 'valid loss', 'accuracy', 'time'])\n",
    "\n",
    "for i, (arch, k) in enumerate(archs):\n",
    "   \n",
    "    model = create_model(arch, dls=dls, **k)\n",
    "    \n",
    "    print(model.__class__.__name__)\n",
    "    \n",
    "    learn = Learner(dls, model,  metrics=accuracy)\n",
    "    start = time.time()\n",
    "    learn.fit_one_cycle(20, 1e-3)\n",
    "    elapsed = time.time() - start\n",
    "    vals = learn.recorder.values[-1]\n",
    "    results.loc[i] = [arch.__name__, k, count_parameters(model), vals[0], vals[1], vals[2], int(elapsed)]\n",
    "    results.sort_values(by='accuracy', ascending=False, ignore_index=True, inplace=True)\n",
    "    clear_output()\n",
    "    display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "\n",
    "torch.save(model, \"test.pt\")\n",
    "\n",
    "\n",
    "\n",
    "model = torch.load(\"test.pt\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "dummy_input = torch.randn(1, 150, 12)\n",
    "\n",
    "input_names = [ \"actual_input\" ]\n",
    "output_names = [ \"output\" ]\n",
    "\n",
    "torch.onnx.export(model.cpu(),\n",
    "                 dummy_input,\n",
    "                 \"cpu.onnx\",\n",
    "                 verbose=False,\n",
    "                 input_names=input_names,\n",
    "                 output_names=output_names,\n",
    "                 export_params=True,\n",
    "                 )\n",
    "\n",
    "\n",
    "onnx_model = onnx.load(\"cpu.onnx\")           # Load the model and check it's ok\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from onnxruntime import InferenceSession\n",
    "onnx_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSAI other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
